:sampleBaseDir: ../../..
:sampleSourceDir: {sampleBaseDir}/src/main/java
:aeronVersion: 1.25.0

= Aeron Cluster Tutorial

ifdef::env-github[]

You appear to be viewing this tutorial on Github.  The document is much nicer to read if you checkout the Aeron project
and run:

[source]
----
$ ./gradlew asciidoctor
----

This will create a more nicely formatted file: `aeron-samples/build/asciidoc/html5/ArchiverTutorial.html` with inline
source code.

endif::[]

IMPORTANT: This tutorial is currently a work in progress, information may be missing or incomplete.

This tutorial assumes that the the user already has a basic working knowledge of Aeron Messaging.

== 1. Introduction

Aeron Cluster is feature that builds upon Aeron and Archive to create a distributed system that contains multiple
redundant instances of the same service whose state is kept synchronized though Aeron Cluster's implementation of the
https://raft.github.io/[Raft Consensus Algorithm].  The base functionality provided is that given a cluster of nodes
(`n`), a message in only processed once a majority of nodes (`⌊n/2⌋ + 1`) have received that message.  This provides
fault tolerance in that a portion of may be unavailable, but the system can safely continue to process messages.

=== Raft Basics

It is not the intention of this tutorial to go into a full description of the https://raft.github.io/[Raft Consensus
Algorithm], however it is difficult explain Aeron Cluster without a few definitions up front.  The key concepts
are (some are Aeron specific):

- Node, a physical server, container instance, VM or group of processes that represents on logicial server within the
cluster.
- Leader, a node within the cluster responsible for replicating messages to the other nodes and waiting for
acknowledgements.
- Follower, other nodes within the cluster that receive messages replicated from the leader.
- Election, process by which the cluster agrees on a new leader.
- Client, node external to the cluster.
- Ingress, the flow messages from a client into the cluster.
- Egress, response mesages from the cluster back to a client.
- Snapshot, serialised representation of the application logic's state.

=== Event Sourcing Basics

Aeron Cluster's fault tolerance models is designed around the concept of event sourcing.  Martin Fowler
https://martinfowler.com/eaaDev/EventSourcing.html[succinctly describes] Event Sourcing as:

[quote]
____
Capture all changes to an application state as a sequence of events.
____

This means that a service, given a known initial state and a reproducible sequence of events can be restored to a state
that it held in the past.  This provides a means to restore a system that has stopped (either crashed or manually
stopped) and create a copies of the state of one service on multiple nodes.

However, it is not quite that simple, in addition to storing initial state and all input events, the service must have
all of its logic implemented deterministically.  Such that all of the output generated for the same set of inputs is the
same.  Ensure that the application logic is deterministic is outside of the control of Aeron Cluster an is the
responsibility of the developer(s) building the application.

[TIP]
====
Be careful to ensure determinism with clustered services, common pitfalls include:

- Timestamps
- Random Numbers
- Iterating over some types of collections, eg. HashMaps.
====

Aeron does provide some assistance in this area, specifically around timestamps, which we will cover later in the
tutorial.

Event Sourcing is incredibly useful as an approach for building reliable in-memory systems, which is common among a
number of very high performance systems, e.g. financial exchanges.  This is because the I/O is limited to a single
append of the incoming event to a log before applying changes the application logic's in-memory state and generating any
associated output events.

In addition to writing all of the messages to a log it is necessary to have a means to take a snapshot of the current
state of the application logic.  The when recovering the system, events are replayed from this snapshot rather than from
the "beginning of time".  Snapshots are generally taken periodically, e.g. daily or hourly. The frequency of the
snapshot should be determined by the volume of data into the system, the throughput of the business logic and the
desired mean time to recovery.  It is not uncommon to have systems that may take an hour or two to recover from a days
worth of messages, in those systems snapshotting every 30 minutes may be more appropriate.

=== Components of Aeron Cluster

One of the key design goals of Aeron is to build a system that is highly composable.  E.g. Aeron Archive which provides
a means to persist an Aeron stream is used by Aeron Cluster to persist the Raft log and messages a communicated around
the cluster using Aeron's Media Driver.  Therefore Aeron Cluster is an aggregation of a number of existing Aeron
components and a few new ones.  To successfully run a cluster node it is necessary to have one (or at least one in the
case of ClusteredServiceContainer) of each of the Aeron components running.  Because all communication between these
components within a single node uses IPC they can be run all in the same process, in separate processes or any arbitrary
combination.

==== MediaDriver

The MediaDriver is the means by which data is moved to, from, and around the cluster.  E.g. Aeron's multicast and
multi-destination-cast functionality is used for offering data into the cluster and ensuring that any active leader
receives it.  Aeron Cluster reuses the Publication and Subscription functionality already provides to handle all
distributed and inter-process communications.

NOTE: Currently  only the Java implementation of the MediaDriver is supported for use in Aeron Cluster.

==== Archive

Raft is primarily a log replication protocol, so Aeron Cluster uses Aeron's Archive functionality as the means to
persist its log.

==== ConsensusModule

The Consensus Module is the key component with Aeron Cluster and provides the functionality for ensuring the nodes have
a consistent copy of the replicated event log.  The Consensus Module will co-ordinate with the Archive to persist
messages, replicate/ack messages to/from other nodes and deliver messages through to the Clustered Services.

==== ClusteredServiceContainer

This is the service that is running the developer supplied application logic.  There can be one or more clustered
services per node.  Aeron Cluster provides a container for the application logic to run within.  There is a
ClusteredService interface that must be implementation by the application that will supply all of the events from the
cluster.

== 2. Getting Started Aeron Cluster

Install JDK 8 or JDK 11.

The simplest way to get started with Aeron is to use the aeron-all jar.  This contains all of the aeron functionality
including Archive and Cluster as well as including the Agrona jar inside of it.  Aeron has no other dependencies.  List
most open source proejects you can use a Maven dependency to pull the code in from Maven central.  The current stable
version is `{aeronVersion}`, however it is recommended that you
https://search.maven.org/artifact/io.aeron/aeron-all[check] to see if there is a more up to date version available.

[source, XML, subs="attributes+"]
----
<dependency>
    <groupId>io.aeron</groupId>
    <artifactId>aeron-all</artifactId>
    <version>{aeronVersion}</version>
</dependency>
----

Aeron is also available as individual jars for each part, taking advantage of that packaging will be described in a
later part of the tutorial.

=== 2.1 Using the Source From this Tutorial

In order to look at the full examples and run the code from this tutorial you will need to checkout the full Aeron
source code.

----
git clone https://github.com/real-logic/aeron.git
----

The location on your computer where this gets checked out to will be referred to as `<AERON_HOME>`.  You will probably
want to build Aeron now to make sure that you have the environment working correctly.

----
cd <AERON_HOME>
./gradlew
----

And to build the tutorial (if you haven't already)

----
./gradlew asciidoctor
----

The code and scripts used in this tutorial are in:
----
<AERON_HOME>/aeron-samples/src/main/java/io/aeron/samples/tutorial/cluster
<AERON_HOME>/aeron-samples/scripts/cluster
----


== 3. Implementing a Clustered Service

The first step to setting up a cluster is to first implement the application logic.  Initially in this tutorial we are
going implement a very simple auction service.  It will have one auction and will track a best price and an id for the
customer that bid that price.  To properly demonstrate the state management and recovery features of cluster it is
important to have some functionality that is stateful, rather than something like an echo service.  For this example so
that we get a good understanding of some the complexities involved, especially around snapshotting we are going to show
all of the gory details of how to handle messages, send responses, snapshotting, and loading.  It is unlikely that
production code would like like this, you would probably want to have a cleaner separation between concerns.  We will
look at examples of that shape in some of the latter sections of the tutorial.  For now we are going to see how the
sausages get made.

First we must define the link between the application logic and the Aeron Cluster.  This is achieved by implementing the
`ClusteredService` interface.

[source, Java, indent=0, options="nowrap"]
----
include::{sampleSourceDir}/io/aeron/samples/tutorial/cluster/BasicAuctionClusteredService.java[tag=new_service]
----

=== Start Up

The `ClusterService` interface defines a number of callbacks that inform us of messages and lifecycle events as they
occur, the first one that concerns us is the `onStart` callback.  This will occur before any input messages, either from
log replay or live from a client.  It is during this phase that we need to load the initial state of the service.  Aeron
Cluster passes in a `Image` that will contain the most recent snapshot of the service.  The service should take care of
deserializing the data from the image and initialize the state of the service.  We will come back to the details of how
the snapshot is loaded.  We should also take a reference to the cluster at this point as we will need it in the future.

[source, Java, indent=0, options="nowrap"]
----
include::{sampleSourceDir}/io/aeron/samples/tutorial/cluster/BasicAuctionClusteredService.java[tag=start]
----

<1> The snapshot can be null (normally this occurs the first time that the service is started).

=== Handling Messages

The `onSessionMessage` callback is the main entry point for requests coming into the cluster.  The cluster will define a
single ingress channel and messages published to this channel will come in through this interface.  This method will
also be called with the messages from the log when it is recovering the system.  One of the other features that Cluster
provides is a reliable timestamp, as mentioned earlier this is one of the challenges of building event sourced systems.
Use this value as the timestamp within your application state and it will be consistent under replay.

[source, Java, indent=0, options="nowrap"]
----
include::{sampleSourceDir}/io/aeron/samples/tutorial/cluster/BasicAuctionClusteredService.java[tag=state]
----
[source, Java, indent=0, options="nowrap"]
----
include::{sampleSourceDir}/io/aeron/samples/tutorial/cluster/BasicAuctionClusteredService.java[tag=message]
----

For our input message we have 3 fields, a `correlationId` which it the customer supplied identifier for the message,
`customerId` to identify the customer placing the bid, and a `price` (we've used a long for this, which represents the
value in cents).

<1> Pull the data out of the message.  This is similar to the pattern used in a Aeron Subscription's `onFragment`
callback when doing a poll.

<2> Execute the business logic, in our this is applying the incoming bid to the auction to see if it is a winner.

<3> The `ClientSession` allows the service to get information about the calling client, but also provide a means to
return responses back to the client.  However the it will be `null` during recovery, so we need to check for that state
and not offer a response in that case.

<4> Serialise response message.

<5> Calling `offer` on the client session will return the response back on the egress channel.  Make sure that the
return value for `offer` is check as it is a non-blocking call and not guaranteed to succeed.

<6> When doing any busy loops within the clustered application use `Cluster::idle(int)` within the wait loop to allow
the service to pause in a friendly manner.  It will take care of handling thread interrupts and ensure the node fails
correctly.

=== Storing State

As was mentioned earlier we need to have a means for our service to regularly take snapshots of its current state in
order to reduce the mean time to recovery and facilitate release migration.  There is a callback
`ClusterService::onTakeSnapshot(ExclusivePublication)` that will be called when it is time to snapshot the state of the
service.

Aeron Cluster provides an `ExclusivePublication` to write a snapshot as the serialised representation of the application
logic's state.  For real world applications snapshotting can become tricky.  The two big concerns you will have will be
ensure that snapshots are written in a consistent manner and dealing with fragmentation of the application state across
messages.  For now, our state is so simple that neither of those will impact use.  However, we will tackle them in a
latter part of the tutorial.

[source, Java, indent=0, options="nowrap"]
----
include::{sampleSourceDir}/io/aeron/samples/tutorial/cluster/BasicAuctionClusteredService.java[tag=takeSnapshot]
----

<1> Write the persistent part of the application logic to a message buffer.  In our case, the currently winning customer
id and bid price.

<2> Write the message to the publication, again we need to check the return from the `offer` call and use
`Cluster::idle` inside of the busy wait loop.

=== Loading State

As you may have noticed in the `onStart` method there was a call to load the snapshot.  Now that we have seen how the
snapshot is written, we can look at how it is loaded.  We also will run until the snapshot image is reached the end.
For a real application, it should also include enough information to determine that it has reached the end of the stored
data.  Then the end of the end of the application data can be matched against the end of the image to sanity the
snaphost.

[source, Java, indent=0, options="nowrap"]
----
include::{sampleSourceDir}/io/aeron/samples/tutorial/cluster/BasicAuctionClusteredService.java[tag=loadSnapshot]
----

<1> We can use the method `Image::isEndOfStream` to determine if there is going to be any more input.

<2> Because our snapshot is stream of messages written to a publication, we use the `Image`'s poll method for extracting
data from the snapshot.

<3> Our total snapshot length (16 bytes) is generally going to be small than any reasonable MTU we can assume that all
of the data will come in a single message.

<4> Once all of the data is loaded we can initialise the application logic state from the snapshot.

<5> Once we've loaded all of the data for the application we can break out of the snapshot loading loop.

<6> Again make sure we use `Cluster::idle` in the tight loops.  It could take time for the snapshot to be propagated to
the service so the number of fragments can be zero.

<7> It is also worthwhile having some sanity checks, these ensure that the snapshot store/load code and Aeron agree on
where the end of the input data is.  Here I've used asserts, but other mechanisms, e.g. log message, exceptions, could
be used to indicate an issue.

=== Other events

There a number other events received by the `ClusteredService` interface, but we are going to ignore them for now and
come back to them later.

=== Summary

As you have seen the with the current example without some clean separation between the application and system the code
can very quickly get messy.  One the primary causes of this is that we have no clear specification for the messages
either those sent from the client or the data being stored in the snapshot.  It is highly recommended that you use some
tool that encapsulates the serialisation of the message data.  Preferably use a tool that allows the message
specification to be represented as schema such that messages data can be decoded without requiring the application code
(e.g Simple Binary Encoding). This will allow a clean separation between the various modules of your application and
between the application and tools used for operation activities like migration, monitoring, and observability.

== Configuring a Cluster

Now we have our application implemented, we can move onto running it in a cluster. Because there are a number of moving
parts to setting up a cluster node, one of the trickiest parts of using Aeron Cluster is getting the configuration
correct.  We are going to start with a static three-node cluster with a simplified configuration where all of the
components (MediaDriver, Archive, ConsensusModule and ClusteredServiceContainer) for a single node are hosted in a
single process.  We will then start the cluster as three separate processes.

=== Running Multiple Nodes on the Same Host

In a production deployment, you will obviously want to run the 3 instances on three separate servers, however for our
example we just want to run the services on a single machine.  This does make port allocation a little bit tricky as
each node within the cluster needs to bind to a number of ports, so we need to make sure that each node has a distinct
set of ports.  We could do this with VMs or containers, but in the interest of simplicity we are going to specify a port
range for each node.  I.e:

- Node 0, ports: 9000-9099
- Node 1, ports: 9100-9199
- Node 2, ports: 9200-9299

[source, Java, indent=0, options="nowrap"]
----
include::{sampleSourceDir}/io/aeron/samples/tutorial/cluster/BasicAuctionClusteredServiceNode.java[tag=ports]
----
[source, Java, indent=0, options="nowrap"]
----
include::{sampleSourceDir}/io/aeron/samples/tutorial/cluster/BasicAuctionClusteredServiceNode.java[tag=udp_channel]
----

While we don't really need 100 ports for each node (closer to 10), it does make it clear which ports are assigned to
each service.  Each endpoint will be an offset from the first port in a node's range.  E.g. the archive control request
port has an offset of 1, so for Node 2 it will have port 9201.

To start the cluster node we are going define our own class that has a main method, it will construct all of contexts
for the necessary components and then start them all.  As indicated above we are going to give each node of the cluster
a unique id, strictly sequential starting at 0.  We are also going to use this value as the cluster member id.

[source, Java, indent=0, options="nowrap"]
----
include::{sampleSourceDir}/io/aeron/samples/tutorial/cluster/BasicAuctionClusteredServiceNode.java[tag=new_service]
//...
----
[source, Java, indent=0, options="nowrap"]
----
//...
include::{sampleSourceDir}/io/aeron/samples/tutorial/cluster/BasicAuctionClusteredServiceNode.java[tag=main]
----

<1> Pass in the node id as a command line parameter to the service so that we can reuse the code for each instance of
the service.

<2> Define a set of hostnames for the cluster.  For now everything is running on localhost, but we could put actual
hostnames or ip addresses in this list an run the same example across multiple servers.

<3> As well as making sure that we don't have any port collisions we don't want our services sharing directories on the
file system, so we use the node id to generate a unique file system location for each node.

<4> Create a shutdown barrier that will be used later to cleanly trap and allow the service to exit.

=== Configuration

==== Media Driver

[source, Java, indent=0, options="nowrap"]
----
include::{sampleSourceDir}/io/aeron/samples/tutorial/cluster/BasicAuctionClusteredServiceNode.java[tag=media_driver]
----

Note we've specified the custom aeronDirName to allow multiple Media Drivers on the same host.  There is nothing special
in the configuration in the Media Driver specific to Cluster.

==== Archive

[source, Java, indent=0, options="nowrap"]
----
include::{sampleSourceDir}/io/aeron/samples/tutorial/cluster/BasicAuctionClusteredServiceNode.java[tag=archive]
----

Again nothing special in the Archive configuration specific to Cluster.  We've used the same aeronDirName as used by the
Media Driver.  The `controlChannel` uses node specific port in the construction of its UDP channel.

=== Archive Client

[source, Java, indent=0, options="nowrap"]
----
include::{sampleSourceDir}/io/aeron/samples/tutorial/cluster/BasicAuctionClusteredServiceNode.java[tag=archive_client]
----

Because Cluster's Consensus Module requires that we write a log file to support he Raft protocol, we need a client for
the constructed Archive for it to use.  The client needs a channel to receive responses back from the cluster, so again
we are using the node specific port to prevent collisions.

=== Consensus Module

[source, Java, indent=0, options="nowrap"]
----
include::{sampleSourceDir}/io/aeron/samples/tutorial/cluster/BasicAuctionClusteredServiceNode.java[tag=consensus_module]
----

This is first of the two configuration sections that specific to Cluster.  The Consensus Module is responsible for
handling the main aspects of the Raft protocol, e.g. leader election, ensuring consensus-based consistency of the data
in the log and passing properly replicated messages onto the Cluster Container.

<1> Each Consensus Module needs an identifier within the Cluster, we are going to use the `nodeId` that we have used to
separate each of the nodes.


<2> This is probably the trickiest part of the configuration.  It specifies all of the static members of the cluster
along with all of the endpoints that they require for the various operations of the Consensus Module.  These are encoded
into a single string of the form.
+
----
0,client-facing:port,member-facing:port,log:port,transfer:port,archive:port| \
1,client-facing:port,member-facing:port,log:port,transfer:port,archive:port| ...
----
Where each of the leading numeric values is a member id for the cluster (as specified in the `clusterMemberId` method).
The values for each endpoint represent:

* client-facing, where the client will connect to for the ingress channel (if using multi-destination cast).
* member-facing, where other members of the cluster will connect to.
* log, used to replicate logs to.
* transfer, used for a stream that members can use to catch up to a leader.
* archive, the same endpoint used to control archive running on this node.
+
In our example we've used the same hostname for each of the endpoints (localhost), however each endpoint allows the
specification of a host, so that traffic could potentially be separated if required, e.g. running member-facing traffic
on a separate network to the client-facing traffic.

<3> Use the node's specific media driver.

<4> Specify the data directory for the Consensus Module.  Make sure it is  node specific.

<5> Define the ingress channel for the cluster.  Note this value does not need to be the full channel URI.  It can be a
template that specifies the parameters, but excludes the endpoint, which will be filled using value from the
clusterMembersString as appropriate for this node.

<6> Specify the log channel.  Like the ingress channel this is specified as a template.  This channel needs to be a
multi-destination cast or multicast channel.  In our example we are using multi-destination cast using manual
configuration, hence we need a separate control endpoint (include a port) used for adding and removing destination to
the publication.

<7> Clone the Archive Client context that the Consensus Module will use to talk to the Archive.

==== Clustered Service Container

The final part of the configuration is the component that will actually run our application logic.  It is possible to
have multiple Clustered Service Containers per Consensus Module and have them talk to each other using IPC.  In our
simplified case we just have the one.

[source, Java, indent=0, options="nowrap"]
----
include::{sampleSourceDir}/io/aeron/samples/tutorial/cluster/BasicAuctionClusteredServiceNode.java[tag=clustered_service]
----

<1> Again we use the node specific Media Driver.

<2> And the node's Archive, via the Archive Client configuration.

<3> This is the point where we bind an instance of our application logic to the cluster.

=== Running the Cluster

Now that the cluster is configured we start running, the code for launching the service is as follows.

[source, Java, indent=0, options="nowrap"]
----
include::{sampleSourceDir}/io/aeron/samples/tutorial/cluster/BasicAuctionClusteredServiceNode.java[tag=running]
----

<1> Launches a `ClusteredMediaDriver` that includes instances of the Media Driver, Archive and Consensus Module.

<2> Immediately afterward we launch a `ClusteredServiceContainer` which is our application.

<3> Use the shutdown barrier to keep our system live until it is signalled to shutdown (e.g. using SIG_TERM or SIG_INT
on Unix).

There is a script provide to launch the cluster.  Assuming that you've followed step 2.1 above you should be able to do
the following:
// TODO need to add windows command examples.

----
cd <AERON_HOME>/aeron-samples/scripts/cluster
./basic-auction-cluster
----

This will spit out a whole lot of logging but should end with something like:

----
CLUSTER: ELECTION_STATE_CHANGE, FOLLOWER_TRANSITION -> FOLLOWER_READY, memberId=2
CLUSTER: ELECTION_STATE_CHANGE, FOLLOWER_CATCHUP -> FOLLOWER_TRANSITION, memberId=1
CLUSTER: ELECTION_STATE_CHANGE, FOLLOWER_TRANSITION -> FOLLOWER_READY, memberId=1
CLUSTER: ELECTION_STATE_CHANGE, FOLLOWER_READY -> CLOSE, memberId=1
CLUSTER: ELECTION_STATE_CHANGE, FOLLOWER_READY -> CLOSE, memberId=2
CLUSTER: ELECTION_STATE_CHANGE, LEADER_READY -> CLOSE, memberId=0
----

We now have our new service running in a cluster.

You can shutdown the nodes using the following:
----
pkill -f BasicAuctionClusteredServiceNode
----

Lets look briefly how the script launches the code.

[source, Bash, indent=0, options="nowrap"]
----
include::{sampleBaseDir}/scripts/cluster/basic-auction-cluster[tag=start_jvm]
----

The `-javaagent:../../../aeron-agent/build/libs/aeron-agent-${VERSION}-all.jar` allows the weaving of the Aeron's
logging agent into the running code.  Specifying `-Daeron.event.cluster.log=all` tells the agent to log all of the
events relating to the cluster operation.

== 4. Using a Cluster Client

While we now have our cluster up and running, we can't do anything with it until we have a client that will iteract with
the cluster.

== Timers

== Multiple Services Per Cluster Node

== More Complex State

== Dynamically Add/Removing Nodes from a Cluster


////
<1> To manage the fragmenting the arbitrary sized state we are going to serialise our state to a single buffer, in our
case we are using the `snapshotMessageBuffer` as our temporary buffer for the keys and their counts.  This makes sense
when the state of the system is as simple as this.  However in a real scenario you probably wouldn't want to serialise
the whole state of a real service to a single buffer, you may want to break up this by
https://dddcommunity.org/resources/ddd_terms/[repository or entity]

<2> You'll notice that we are sorting the output before writing to the buffer.  While not strictly required, it is
highly recommended that snapshots be written with data in a consistent order.  This means that if at some point in the
future you wanted to verify snapshots written by different services that have logically consumed the same messages then
it can be done as a straight forward binary comparison.  A sort is required here because `HashMap` iteration is
no-deterministic (especially in cases where services are at the same point in the log, but initialised from snapshots
taken at different times).  An alternative would be to use a collection that has a known order, e.g. `TreeMap`.

<3> Two things are happening here.  Firstly we mentioned the necessity of having a protocol to handle fragmentation of
our data when sending via the `Publication`.  In our case we are going to have a very minimal header, consisting of two
32 bit values, the current offset in to the full buffer of data and the total size.  This will allow a consumer to know
how far through the total set of data this fragment is and can be used for sanity checking on load.  Secondly we are
determining the size of the individual Aeron message be sent.  Given that we will be handling fragmentation ourselves,
there is no point in having Aeron fragement and reconstruct messages for us, so we should limit the size of the messages
such that we do not exceed the `MTU`, so this calculate tells us the maximum amount of the encoded
`snapshotMessageBuffer` we can send with each message.

<4> Write out our header to the first two 32 bit entries in the fragment.

<5> Get the amount of data from the full serialised message that we want to send, either fill up the MTU or use whatever
is remaining, whichever is smallest.

<6> Write the header and the message data to the publication using the gathering API.

Before moving onto configuring our service to run in a cluster, a couple of points to remember.

[TIP]
====
. Treat snapshot store and load as a messaging problem using a protocol to efficiently deal with fragmentation.
. Ensure message serialisation is encapsulated, preferably with a tool that provides a schema for the serialised data.
. Write data to the snapshot in a deterministic order to allow for verification later.
====
////